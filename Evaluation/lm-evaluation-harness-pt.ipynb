{"cells":[{"cell_type":"markdown","metadata":{"id":"Ac6wadk3rmkK"},"source":["# LM Evaluation Harness (by [EleutherAI](https://www.eleuther.ai/) & [Laiviet](https://github.com/laiviet/lm-evaluation-harness))\n","\n","<a href=\"https://colab.research.google.com/drive/1mspcStRItqKzLZ39PG-ztKJXCqSvlEKt\" target=\"_blank\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n","</a>\n","\n","This [`LM-Evaluation-Harness`](https://github.com/EleutherAI/lm-evaluation-harness) provides a unified framework to test generative language models on a large number of different evaluation tasks. For a complete list of available tasks, scroll to the bottom of the page.\n","\n","1. Clone the [lm-evaluation-harness](https://github.com/laiviet/lm-evaluation-harness) and install the necessary libraries (`sentencepiece` is required for the Llama tokenizer)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UA5I86u91e0A"},"outputs":[],"source":["!git clone --branch main https://github.com/laiviet/lm-evaluation-harness.git\n","!cd lm-evaluation-harness && pip install -e . -q\n","!pip install cohere tiktoken sentencepiece -q"]},{"cell_type":"markdown","metadata":{},"source":["2. Run the evaluation on the selected tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnHoAVK25QZn"},"outputs":[],"source":["!huggingface-cli login --token your_token\n","!cd lm-evaluation-harness && python main.py \\\n","    --model hf-auto \\\n","    --model_args pretrained=nicholasKluge/TeenyTinyLlama-460m \\\n","    --tasks arc_pt,hellaswag_pt,mmlu_pt,truthfulqa_pt  \\\n","    --device cuda:0 \\\n","    --model_alias TTL \\\n","    --task_alias open_llm"]},{"cell_type":"markdown","metadata":{"id":"4Bm78wiZ4Own"},"source":["## Task Table ðŸ“š\n","\n","| Task Name      | Train | Val | Test | Val/Test Docs | Metrics       |\n","|----------------|-------|-----|------|---------------|---------------|\n","| arc_pt,mmlu_pt | âœ“     | âœ“   | âœ“    | 1172         | acc, acc_norm |\n","| hellaswag_pt   | âœ“     | âœ“   |      | 10042        | acc, acc_norm |\n","| mmlu_pt        |       | âœ“   | âœ“    | 1,662        | acc, acc_norm |\n","| truthfulqa_pt  |       | âœ“   |      | 817          | mc1, mc2      |  "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMESD8bzbpe10qjgivZFHol","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
